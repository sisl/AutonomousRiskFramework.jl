### A Pluto.jl notebook ###
# v0.14.2

using Markdown
using InteractiveUtils

# This Pluto notebook uses @bind for interactivity. When running this notebook outside of Pluto, the following 'mock version' of @bind gives bound variables a default value (instead of an error).
macro bind(def, element)
    quote
        local el = $(esc(element))
        global $(esc(def)) = Core.applicable(Base.get, el) ? Base.get(el) : missing
        el
    end
end

# â•”â•â•¡ e1cc5eae-c7dd-4e14-ac53-c76dc83cf831
begin
	function ingredients(path::String)
		# this is from the Julia source code (evalfile in base/loading.jl)
		# but with the mod. that it returns the module instead of the last object
		name = Symbol(basename(path))
		m = Module(name)
		Core.eval(m,
			Expr(:toplevel,
				 :(eval(x) = $(Expr(:core, :eval))($name, x)),
				 :(include(x) = $(Expr(:top, :include))($name, x)),
				 :(include(mapexpr::Function, x) =
					$(Expr(:top, :include))(mapexpr, $name, x)),
				 :(include($path))))
		return m
	end

	# using PyPlot, Seaborn
	ast = ingredients("../ObservationModels/examples/organized_ast.jl")
	plotting = ingredients("../RiskSimulator.jl/src/visualization/plotting.jl")

	using Distributions
	using PlutoUI
end

# â•”â•â•¡ ce186b02-7e6a-4f46-8d5b-548a3689eebe
using MixFit

# â•”â•â•¡ 345067fa-8d53-4847-944d-e373f1c0dd01
md"""
## End-to-End Autonomous Risk Framework
A demo of the end-to-end risk assessment framework comparing the Intelligent Driver Model (IDM) with the Princeton Driver Model.
"""

# â•”â•â•¡ 696414a6-f2ac-437f-a0a0-c8e13d35377e
md"""
# Intelligent Driver Model
"""

# â•”â•â•¡ 7e0a2028-5b33-4050-9b3e-ef7f29c2de3d
md"""
Set RNG seeds for determinism.
"""

# â•”â•â•¡ 929c42ed-11c3-48f9-ae8a-74cbff0f8586
begin
	SEED = 1000
	ast.Random.seed!(SEED);
end;

# â•”â•â•¡ 0aabf8be-93fa-4022-afb9-d381dd14ba5d
md"""
## Phase 1: Observation model fitting
"""

# â•”â•â•¡ 12adfc80-775b-4105-8fff-52d4d5e4d274
md"""
Train a surrogate model (using a neural network in this case) to replace the observation model.
"""

# â•”â•â•¡ d89e1d69-9339-4c68-a1db-c87bb463ee12
SEED; net, net_params = ast.training_phase();

# â•”â•â•¡ a58bb08d-1507-41d4-b9d5-620e52700a67
md"""
## Phase 2: Standard AST Failure Search
"""

# â•”â•â•¡ 89c59d7d-d0d6-425f-8cdb-b4b10cc0a211
md"""
1. Select system under test (i.e., autonomous vehicle platform)
"""

# â•”â•â•¡ 727b5568-89ec-4c98-b6b3-08aa03442777
system = ast.IntelligentDriverModel(v_des=12.0);

# â•”â•â•¡ d54a2257-8ff2-4ce5-b80e-ba2434e14d2f
md"""
2. Set-up AST search problem.
"""

# â•”â•â•¡ f3459a95-fd43-495a-8c5b-362dc4f35536
planner = ast.setup_ast(net, net_params; sut=system, seed=SEED);

# â•”â•â•¡ 5308a9c1-82fb-424b-bb0b-26d2fe64b04a
md"""
3. Run AST search.
"""

# â•”â•â•¡ 18d03acb-5dcf-474f-a8cb-2c7f96ad99af
action_trace = ast.search!(planner);

# â•”â•â•¡ f599cb73-cd85-4f6e-96f6-c63e6802a03c
md"""
#### Failure metrics
We can capture **failure metrics**:
- failure rate (i.e., biased $p(\text{fail})$)
- first failure index (i.e., "ease" of finding failures)
- number of failures
- highest log-likelihood of failure
"""

# â•”â•â•¡ 65e3ea70-493f-4d48-a056-daec44993e01
failure_metrics = ast.failure_metrics(planner);

# â•”â•â•¡ 0f7dd3f7-e595-4d4f-9770-387b9f58ad32
ast.POMDPStressTesting.latex_metrics(failure_metrics)

# â•”â•â•¡ 6f302f7c-e2fd-4267-a8e6-57ea493c2699
md"""
Let $\alpha$ be the risk threshold over cost distribution (i.e., severity of failure distribution).
"""

# â•”â•â•¡ 746c7128-c854-4cdd-82a8-0566287b1c35
Î± = 0.2;

# â•”â•â•¡ 0a40b9d0-835a-4986-85b3-26a2ee96aeea
md"""
We accept $\alpha$ percentage of risk, so when changing $\alpha$:
- If we lower $\alpha$, we are saying "we want to accept _less_ risk", so the area-under-curve (AUC) will be larger, i.e., more measured risk.

- If we increase $\alpha$, we can accept _more_ risk, so the AUC will be smaller, i.e., less measured risk. 
"""

# â•”â•â•¡ 02ff1404-1914-44da-a5ea-de59e854f702
md"""
Collect cost value $Z$ (i.e., severity of failure). We use the _rate_ value already collected and used by AST:

$$\text{rate} = d_{t+1} - d_t$$

where $d$ is the miss distance metric already collected by AST (preserving the black-box assumption).
"""

# â•”â•â•¡ 7560d798-3e9d-4800-9f47-73c9b03eed8f
ğ’Ÿ = planner.mdp.dataset;

# â•”â•â•¡ a43e2484-ff74-4d7b-a3e7-fc13cb9558a1
plotting.plot_closure_rate_distribution(ğ’Ÿ; reuse=false)

# â•”â•â•¡ 9045b21a-fc55-414e-a490-5f568a22efb0
md"""
#### Risk metrics
Now we can also collect the **risk metrics**:
- Expected cost $\mathbb{E}[Z]$ (i.e., mean cost)
- Value at Risk $\operatorname{VaR}$
- Conditional Value at Risk $\operatorname{CVaR}$
- Worst case cost
"""

# â•”â•â•¡ 9641d717-229e-48cc-a105-455ea250ddaf
md"""
Collect cost/severity data $Z$.
"""

# â•”â•â•¡ 2538080d-ea56-46ae-b4eb-ecf4a2807b6b
Z = ast.cost_data(ğ’Ÿ);

# â•”â•â•¡ 9c250ac1-ddf9-46ee-9d53-a43ff55ca765
metrics = ast.RiskMetrics(Z, Î±);

# â•”â•â•¡ 9c54880f-a81d-456f-aad4-b89bc429f2c8
ast.latex_metrics(metrics)

# â•”â•â•¡ 347023a3-4f9a-4f1d-8def-1c88d955301e
plotting.plot_risk(metrics; mean_y=0.33, var_y=0.25, cvar_y=0.1, Î±_y=0.2)

# â•”â•â•¡ cc812055-735b-4fb6-9300-d5c14a3531a0
md"Vizualize failure? $(@bind viz CheckBox())"

# â•”â•â•¡ de9ffff5-93f5-4cd7-a402-4d6b81b8fee1
viz && ast.visualize_most_likely_failure(planner, ast.buildingmap);

# â•”â•â•¡ 06691cee-80cf-4863-a52f-4ad6aeff8deb
md"""
# Princeton Driver Model
"""

# â•”â•â•¡ b0f742ab-1144-4a77-bea7-1ad28bcfba42
md"""
## Phase 2: Standard AST Failure Search
"""

# â•”â•â•¡ 4993fbcb-8ce6-4b2b-9001-c774825b0811
md"""
Now we can change the SUT to use the Princeton Driver Model.
"""

# â•”â•â•¡ 791b9070-c99e-4df1-9d56-ffe01aaed609
system2 = ast.PrincetonDriver(v_des=2.2);

# â•”â•â•¡ 4e4801f5-7125-4d1b-bb14-650254ac68a9
md"""
And set-up our AST search problem, passing a different value for `sut`.
"""

# â•”â•â•¡ 68d0160f-91c2-4a99-afd5-c5548b668c4a
planner2 = ast.setup_ast(net, net_params; sut=system2, seed=SEED);

# â•”â•â•¡ 40eca7fb-cbe4-4908-a522-d6747d46efe1
md"""
And finally, run the AST search.
"""

# â•”â•â•¡ 2252129b-407d-4df3-ad6d-bd7fff7d29ce
action_trace2 = ast.search!(planner2);

# â•”â•â•¡ a82c4d5e-8beb-4e91-ae2e-07972d1a8882
ğ’Ÿ2 = planner2.mdp.dataset;

# â•”â•â•¡ 0f93a146-1a9e-466b-b44a-e6a702d076cf
plotting.plot_closure_rate_distribution(ğ’Ÿ2; reuse=false)

# â•”â•â•¡ 01c76e6f-caca-4042-a70f-2516c6f66f36
md"""
Collect **failure metrics**.
"""

# â•”â•â•¡ 8d77bc0f-f78e-4b80-901b-69a8d0770e2f
failure_metrics2 = ast.failure_metrics(planner2);

# â•”â•â•¡ 888cb1a8-f18f-460e-ae79-93505c45045e
ast.POMDPStressTesting.latex_metrics(failure_metrics2)

# â•”â•â•¡ dd0b95ed-7955-4f0e-bd0c-8ed62bcce9cf
md"""
Collect **risk metrics**.
"""

# â•”â•â•¡ c3d482ba-7129-40eb-8dbc-d46b1bee04d7
Z2 = ast.cost_data(planner2.mdp.dataset);

# â•”â•â•¡ fffdded8-69a1-4361-90a8-1a9de0256965
metrics2 = ast.RiskMetrics(Z2, Î±);

# â•”â•â•¡ f30bd6a9-7fdf-448c-b28b-662b5afe8e5a
ast.latex_metrics(metrics2)

# â•”â•â•¡ 5046d560-2deb-4702-8272-bc9b27ef8462
plotting.plot_risk(metrics2; mean_y=0.18, var_y=0.15, cvar_y=0.1, Î±_y=0.13)

# â•”â•â•¡ c1ded51d-dacd-4908-90a7-99a20283ea19
md"Vizualize failure? $(@bind viz2 CheckBox())"

# â•”â•â•¡ ce5ee9cf-f614-4b48-b1e1-58791f7256ad
viz2 && ast.visualize_most_likely_failure(planner2, ast.buildingmap);

# â•”â•â•¡ 4f29753c-f0e3-42be-baab-a06da23a0255
md"""
# Overall Risk Measure
Using the risk metrics (i.e., mean cost, VaR, CVaR, and worst case cost), we can compute a polar plot to visualize the overall risk.

We can adjust the weight of each metrics using the vector $\mathbf{w}$.
"""

# â•”â•â•¡ e71c6ef2-09b2-40e6-ae42-08810de2a01a
ğ° = [1, 1, 1, 1];

# â•”â•â•¡ 416b0fc0-d644-48a9-ad06-a6dfb7accc5c
plotting.plot_risk_metrics([metrics, metrics2], ["IDM", "Princeton"]; weights=ğ°)

# â•”â•â•¡ c2cfad18-9ee5-412c-be31-89b9ab4b0aea
md"""
Compute the area under each curve (higher = more risky).
"""

# â•”â•â•¡ 241fc15d-07a8-486e-9d16-4b2e95941f1f
plotting.risk_area([metrics, metrics2]; weights=ğ°)

# â•”â•â•¡ 75b14899-2165-4b04-a1aa-6298e40380c8
md"""
Now using _all_ metrics (i.e., risk metrics _and_ failure metrics from AST), we can recompute the polar plots and compute each AV's risk measure as the area under the curve.
"""

# â•”â•â•¡ 09c827a6-7782-4449-9953-df585514b72d
ğ°â€² = [1, 1, 1, 1, 1, 1, 1];

# â•”â•â•¡ f9c101cc-6861-4ac3-95f9-a96a6fa757f7
plotting.plot_overall_metrics([planner,planner2], ["IDM","Princeton"]; weights=ğ°â€²,Î±=Î±)

# â•”â•â•¡ ed25fe07-36f3-4264-99ba-6181eb4adca9
md"""
Re-calculate the risk area using _both_ risk metrics and failure metrics.
"""

# â•”â•â•¡ 70c38db1-229c-4bd0-89b1-6621d97f05f3
plotting.overall_area([planner, planner2]; weights=ğ°â€², Î±=Î±)

# â•”â•â•¡ d86498f0-4162-440a-8c15-9973bb508116
md"""
> **Notice** how the overall risk is different when including the _failure metrics_ combined with the _risk metrics_. The weights $\mathbf{w}'$ can be adjusted to balance how much each metric affects the overall risk.
"""

# â•”â•â•¡ a58e787e-6082-4971-b445-c9fc18898ae0
md"""
# Distribution of failure likelihood
"""

# â•”â•â•¡ 8c5b2c05-6ef4-4f5c-b964-d795ed0a3369
md"Display likelihood distribution? $(@bind llviz CheckBox())"

# â•”â•â•¡ 7da84a62-979b-4b25-963c-9d6f461bcefa
llviz && using PyPlot, Seaborn, Revise, POMDPStressTesting

# â•”â•â•¡ 21705423-986b-4c58-abc2-a697d7f4ded6
if llviz
	distribution_figures(planner.mdp.metrics; label="IDM")
	distribution_figures!(planner2.mdp.metrics; color="purple", label="Princeton")
end

# â•”â•â•¡ 83525435-68c1-436e-8cd0-5e8fc0930eae
md"""
# Fitting and predictions
"""

# â•”â•â•¡ c0bd451e-7667-4194-ad36-be05ecf16259
md"""
## Quadratic discriminant analysis (QDA)

To predict a failure given a rate $r$ and distance $d$ as $x=[r,d]$, we can use quadratic discriminant analysis (QDA):$^{[1]}$

$$\delta_k(x) = -\frac{1}{2}\mu_k\Sigma_k^{-1}\mu_k + x^\top\Sigma_k^{-1}\mu_k - \frac{1}{2}x^\top\Sigma_k^{-1}x - \frac{1}{2}\log|\Sigma_k|$$

and when $\delta_1(x) > \delta_2(x)$, then we classify as _failure_ (i.e., class 1).

We can also set $\delta_1(x) = \delta_2(x)$ and solve for $\delta_1(x)$ to get:

$$\delta(x) = (x - \mu_1)^\top\Sigma_1^{-1}(x - \mu_1) + \log(|\Sigma_1|) - (x - \mu_2)^\top\Sigma_2^{-1}(x - \mu_2) - \log(|\Sigma_2|)$$

and when $\delta(x) < 0$, then we classify as _failure_ (i.e., class 1).
"""

# â•”â•â•¡ b6abf576-dfb1-4924-a171-09a9d0326851
 plotting2 = ingredients("../RiskSimulator.jl/src/visualization/plotting.jl")

# â•”â•â•¡ d005ecc7-36dc-4b27-933e-c6b55152be13
 plotting3 = ingredients("../RiskSimulator.jl/src/visualization/plotting.jl")

# â•”â•â•¡ 63c817ac-58b9-4304-87d1-4c7065af2393
qda_fig_linear = plotting2.plot_multivariate_distance_and_rate(ğ’Ÿ;
	gda=true,
	gdatype=:qda,
	svm=true,
	boundary=:binary,
	subplots=false); savefig("qda_linear_boundary.pdf", bbox_inches = "tight",
    pad_inches = 0.1); qda_fig_linear

# â•”â•â•¡ f772cb78-6939-4919-b4e0-7749029e9932
md"""
### QDA for classification
"""

# â•”â•â•¡ 52f940e7-ba00-4bcb-8f6c-7643ce9b4646
qda_fig, predict_qda = plotting2.plot_multivariate_distance_and_rate(ğ’Ÿ;
	gda=true,
	gdatype=:qda,
	svm=true,
	boundary=:gradient,
	return_predict=true,
	subplots=false); savefig("qda.pdf", bbox_inches = "tight",
    pad_inches = 0.1); qda_fig

# â•”â•â•¡ 0f4b36fc-79bb-46e7-89b2-6dcd6a5c58be
classify(x, pred) = pred(x) < 0 ? :failure : :non_failure;

# â•”â•â•¡ fd43367b-a33c-4c09-99e1-ecb7a8f1b293
show_prediction(x, pred) = println(x, "\t=> ", pred(x), "\t=> ", classify(x, pred));

# â•”â•â•¡ 308441b8-b485-433f-b233-91de7da3b7e7
with_terminal() do
	show_prediction([0.2, 4], predict_qda)
	show_prediction([0.2, 5], predict_qda)
end

# â•”â•â•¡ d20eba0f-6720-49ec-8f8f-e3f37c555f79
md"""
## Linear discriminant analysis (LDA)

If we (incorrectly) assume the covariances are the same (i.e., $\Sigma_1 = \Sigma_2 = \Sigma$), then we get linear discriminant analysis (LDA):

$$\delta_k(x) = x^\top\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k\Sigma^{-1}\mu_k + \log(\pi_k)$$

and when $\delta_1(x) > \delta_2(x)$, then we classify as _failure_ (i.e., class 1).

Similarly, we can also set $\delta_1(x) = \delta_2(x)$ and solve for $\delta_1(x)$ to get:

$$\delta(x) = (x - \mu_1)^\top\Sigma^{-1}(x - \mu_1) - (x - \mu_2)^\top\Sigma^{-1}(x - \mu_2)$$

and if $\delta(x) < 0$, then we classify as _failure_.
"""

# â•”â•â•¡ 06241562-1ab8-4a6f-92ce-722a607a6d6d
lda_fig_linear, predict_lda_linear = plotting3.plot_multivariate_distance_and_rate(ğ’Ÿ;
	gda=true,
	gdatype=:lda,
	svm=true,
	boundary=:binary,
	return_predict=true,
	subplots=true); savefig("lda_linear_boundary.pdf", bbox_inches = "tight",
    pad_inches = 0.1); lda_fig_linear

# â•”â•â•¡ d20b6d40-0d60-4208-9aa1-76a7493418c5
predict_lda_linear([0.1, 4])

# â•”â•â•¡ 88a3ef1f-a1fb-4965-9fc9-0656573897f1
lda_fig, predict_lda = plotting2.plot_multivariate_distance_and_rate(ğ’Ÿ;
	gda=true,
	gdatype=:lda,
	svm=true,
	boundary=:gradient,
	return_predict=true,
	subplots=false); savefig("lda.pdf", bbox_inches = "tight",
    pad_inches = 0.1); lda_fig

# â•”â•â•¡ 296c3373-2cab-49d6-a173-94e4fec021a8
predict_lda([0.1, 4])

# â•”â•â•¡ 3ff338ac-cabb-4cc1-a326-282562901318
with_terminal() do
	show_prediction([0.2, 4], predict_lda)
	show_prediction([0.2, 5], predict_lda)
end

# â•”â•â•¡ 11d024b2-2a12-4ebe-9c79-96e19028cdf1
begin
	Z_fail = ast.cost_data(ğ’Ÿ)
    Z_nonfail = ast.cost_data(ğ’Ÿ; nonfailures=true)
    d_fail = ast.distance_data(ğ’Ÿ)
    d_nonfail = ast.distance_data(ğ’Ÿ; nonfailures=true)
    
    pos_data = [Z_fail d_fail]'
    neg_data = [Z_nonfail d_nonfail]'
end

# â•”â•â•¡ c18d944a-56b1-45bd-b974-ad08ad44df9c
pos_data'

# â•”â•â•¡ 98aebd62-ba14-481b-9c28-97b9abf4bc4f
Ïƒ(z) = 1 / (1 + exp(-z))

# â•”â•â•¡ d56c10a8-4feb-4621-a005-0206cfe0f14a
-predict_qda([0.5,3]) |> tanh # Ïƒ

# â•”â•â•¡ 2b6a3941-5d03-4b3a-a8e8-1b3ebbc62f20
predict_qda([0.5, 3])

# â•”â•â•¡ e5318d56-6289-4451-ac03-f5119223990a
svm_boundary = (x,w,b) -> (-w[1] * x .+ b)/w[2] # line of the decision boundary

# â•”â•â•¡ 4942de35-2a00-4419-b959-b2caa90b361b
w, b = plotting2.compute_svm(pos_data, neg_data)

# â•”â•â•¡ 2051d694-9864-4341-ace4-5ad0184c441a
svm_boundary(0, w, b)

# â•”â•â•¡ 1ca3e292-ce0a-4cef-a08b-6d6628bad2c3
svm_classify(x) = sign((-w'*x + b)/w[2]) > 0 ? 1 : 0

# â•”â•â•¡ ce934e7b-b915-4730-92e4-24f31bd1a745
svm_classify([0,4.08])

# â•”â•â•¡ a95f7059-f862-4284-bfb2-17a753136e8b
sum([svm_classify(x) for x in eachcol(pos_data)]) / size(pos_data)[2]

# â•”â•â•¡ 02a4fa64-c9c1-4b1d-9ce3-0128317b360a
sum([1-svm_classify(x) for x in eachcol(neg_data)]) / size(neg_data)[2]

# â•”â•â•¡ 32fefe1e-7ec0-4756-afa8-70f2d0ae9b25
sum([predict_qda(x) < 0 ? 1 : 0 for x in eachcol(pos_data)]) / size(pos_data)[2]

# â•”â•â•¡ 6903aa6c-26ca-4c6f-8dcd-45e9bb7a97b8
sum([1-predict_qda(x) < 0 ? 1 : 0 for x in eachcol(neg_data)]) / size(neg_data)[2]

# â•”â•â•¡ 8009fcbf-892f-445b-a12d-3d2470c71a68
md"""
## Efficient Validation using Predictive Risk
"""

# â•”â•â•¡ b7c80bcd-664a-49e5-a9f4-c84164a213be
begin
	planner3 = ast.setup_ast(net, net_params; sut=system, seed=SEED);
	planner3.mdp.predict = x -> -predict_qda(x) # NOTE negation.
	ast.search!(planner3);
	failure_metrics3 = ast.failure_metrics(planner3);
	ast.POMDPStressTesting.latex_metrics(failure_metrics3)
end

# â•”â•â•¡ 67188719-ebb0-4dc1-8bcd-36ab2f6f1870
# 0.18 (normal), 0.51 (LDA), 0.65 (QDA)

# â•”â•â•¡ 312fec2e-94c3-4beb-8e29-9a6a417541c3
md"""
# Multivariate Gamma Fits
"""

# â•”â•â•¡ a6bc299e-deb9-4bb7-8661-52657377988a
figure(); scatter(pos_data[1,:], pos_data[2,:]); gcf()

# â•”â•â•¡ 0fa42aa3-88b8-4786-a619-8bea71f6be8d
# Wishart(2, 

# â•”â•â•¡ 632a6f89-1528-4142-820e-adeb9d41e48e
G = pos_data * pos_data'

# â•”â•â•¡ 60d7695c-63cc-4757-807a-a1e294a86360
W = Wishart(2, G)

# â•”â•â•¡ 601932f0-a0b1-4fdc-801f-e01b08087e43
pdf(W, [0.00001 0; 0 0.00000001])

# â•”â•â•¡ 09272854-da18-417c-ab4c-a3db469f4c92
fit_mle(W, pos_data)

# â•”â•â•¡ c3be7332-7451-4fba-a459-835b36f420f6
Î“â‚ = fit_mle(Gamma, pos_data[1,:])

# â•”â•â•¡ dc8afbce-17dd-470b-8ab3-5023605537d6
Î“â‚‚ = fit_mle(Gamma, pos_data[2,:])

# â•”â•â•¡ 721a32bd-1a1a-4f1f-bf1f-ffc09fcde6e2
pos_data

# â•”â•â•¡ ce68a804-8cd1-4932-8b76-67b5b863ef75
pdfÎ“(ğšª, ğ±) = pdf(ğšª[1], ğ±[1]) * pdf(ğšª[2], ğ±[2])

# â•”â•â•¡ 6dc2d711-4286-429f-97df-c360d7846e2b
pdfÎ“([Î“â‚, Î“â‚‚], [0,4])

# â•”â•â•¡ 1a936087-22f7-427a-829b-8f328e71046d
begin
	figure()
	scatter(pos_data[1,:], pos_data[2,:])
	current_xlim = xlim()
	current_ylim = ylim()
    fX = range(-2, stop=current_xlim[2], length=1000)
    fY = range(-2, stop=current_ylim[2], length=1000)
    fZ = [pdfÎ“([Î“â‚, Î“â‚‚], [x,y]) for y in fY, x in fX] # Note x-y "for" ordering
    
    contour(fX, fY, fZ, alpha=0.75, cmap="plasma")
	xlim(current_xlim)
	ylim(current_ylim)
	gcf()
end

# â•”â•â•¡ acbfdf1f-dcd8-4d2c-bcfc-ad221a808134
MF = mixfit([pos_data[1,:]; pos_data[2,:]], 2)

# â•”â•â•¡ cd71d1f6-059a-4b65-ba40-57c02386998b
MF.Î¼

# â•”â•â•¡ 046e4943-61c2-4258-b876-8b898819cd5b
md"""
---
[1]: T. Hastie, R. Tibshirani, and J. Friedman, _The Elements of Statistical Learning_, Springer, 2001.
"""

# â•”â•â•¡ Cell order:
# â• â•e1cc5eae-c7dd-4e14-ac53-c76dc83cf831
# â•Ÿâ”€345067fa-8d53-4847-944d-e373f1c0dd01
# â•Ÿâ”€696414a6-f2ac-437f-a0a0-c8e13d35377e
# â•Ÿâ”€7e0a2028-5b33-4050-9b3e-ef7f29c2de3d
# â• â•929c42ed-11c3-48f9-ae8a-74cbff0f8586
# â•Ÿâ”€0aabf8be-93fa-4022-afb9-d381dd14ba5d
# â•Ÿâ”€12adfc80-775b-4105-8fff-52d4d5e4d274
# â• â•d89e1d69-9339-4c68-a1db-c87bb463ee12
# â•Ÿâ”€a58bb08d-1507-41d4-b9d5-620e52700a67
# â•Ÿâ”€89c59d7d-d0d6-425f-8cdb-b4b10cc0a211
# â• â•727b5568-89ec-4c98-b6b3-08aa03442777
# â•Ÿâ”€d54a2257-8ff2-4ce5-b80e-ba2434e14d2f
# â• â•f3459a95-fd43-495a-8c5b-362dc4f35536
# â•Ÿâ”€5308a9c1-82fb-424b-bb0b-26d2fe64b04a
# â• â•18d03acb-5dcf-474f-a8cb-2c7f96ad99af
# â•Ÿâ”€f599cb73-cd85-4f6e-96f6-c63e6802a03c
# â• â•65e3ea70-493f-4d48-a056-daec44993e01
# â• â•0f7dd3f7-e595-4d4f-9770-387b9f58ad32
# â•Ÿâ”€6f302f7c-e2fd-4267-a8e6-57ea493c2699
# â• â•746c7128-c854-4cdd-82a8-0566287b1c35
# â•Ÿâ”€0a40b9d0-835a-4986-85b3-26a2ee96aeea
# â•Ÿâ”€02ff1404-1914-44da-a5ea-de59e854f702
# â• â•7560d798-3e9d-4800-9f47-73c9b03eed8f
# â• â•a43e2484-ff74-4d7b-a3e7-fc13cb9558a1
# â•Ÿâ”€9045b21a-fc55-414e-a490-5f568a22efb0
# â•Ÿâ”€9641d717-229e-48cc-a105-455ea250ddaf
# â• â•2538080d-ea56-46ae-b4eb-ecf4a2807b6b
# â• â•9c250ac1-ddf9-46ee-9d53-a43ff55ca765
# â• â•9c54880f-a81d-456f-aad4-b89bc429f2c8
# â• â•347023a3-4f9a-4f1d-8def-1c88d955301e
# â•Ÿâ”€cc812055-735b-4fb6-9300-d5c14a3531a0
# â• â•de9ffff5-93f5-4cd7-a402-4d6b81b8fee1
# â•Ÿâ”€06691cee-80cf-4863-a52f-4ad6aeff8deb
# â•Ÿâ”€b0f742ab-1144-4a77-bea7-1ad28bcfba42
# â•Ÿâ”€4993fbcb-8ce6-4b2b-9001-c774825b0811
# â• â•791b9070-c99e-4df1-9d56-ffe01aaed609
# â•Ÿâ”€4e4801f5-7125-4d1b-bb14-650254ac68a9
# â• â•68d0160f-91c2-4a99-afd5-c5548b668c4a
# â•Ÿâ”€40eca7fb-cbe4-4908-a522-d6747d46efe1
# â• â•2252129b-407d-4df3-ad6d-bd7fff7d29ce
# â• â•a82c4d5e-8beb-4e91-ae2e-07972d1a8882
# â• â•0f93a146-1a9e-466b-b44a-e6a702d076cf
# â•Ÿâ”€01c76e6f-caca-4042-a70f-2516c6f66f36
# â• â•8d77bc0f-f78e-4b80-901b-69a8d0770e2f
# â• â•888cb1a8-f18f-460e-ae79-93505c45045e
# â•Ÿâ”€dd0b95ed-7955-4f0e-bd0c-8ed62bcce9cf
# â• â•c3d482ba-7129-40eb-8dbc-d46b1bee04d7
# â• â•fffdded8-69a1-4361-90a8-1a9de0256965
# â• â•f30bd6a9-7fdf-448c-b28b-662b5afe8e5a
# â• â•5046d560-2deb-4702-8272-bc9b27ef8462
# â•Ÿâ”€c1ded51d-dacd-4908-90a7-99a20283ea19
# â• â•ce5ee9cf-f614-4b48-b1e1-58791f7256ad
# â•Ÿâ”€4f29753c-f0e3-42be-baab-a06da23a0255
# â• â•e71c6ef2-09b2-40e6-ae42-08810de2a01a
# â• â•416b0fc0-d644-48a9-ad06-a6dfb7accc5c
# â•Ÿâ”€c2cfad18-9ee5-412c-be31-89b9ab4b0aea
# â• â•241fc15d-07a8-486e-9d16-4b2e95941f1f
# â•Ÿâ”€75b14899-2165-4b04-a1aa-6298e40380c8
# â• â•09c827a6-7782-4449-9953-df585514b72d
# â• â•f9c101cc-6861-4ac3-95f9-a96a6fa757f7
# â•Ÿâ”€ed25fe07-36f3-4264-99ba-6181eb4adca9
# â• â•70c38db1-229c-4bd0-89b1-6621d97f05f3
# â•Ÿâ”€d86498f0-4162-440a-8c15-9973bb508116
# â•Ÿâ”€a58e787e-6082-4971-b445-c9fc18898ae0
# â•Ÿâ”€8c5b2c05-6ef4-4f5c-b964-d795ed0a3369
# â• â•7da84a62-979b-4b25-963c-9d6f461bcefa
# â• â•21705423-986b-4c58-abc2-a697d7f4ded6
# â•Ÿâ”€83525435-68c1-436e-8cd0-5e8fc0930eae
# â•Ÿâ”€c0bd451e-7667-4194-ad36-be05ecf16259
# â• â•b6abf576-dfb1-4924-a171-09a9d0326851
# â• â•d005ecc7-36dc-4b27-933e-c6b55152be13
# â• â•63c817ac-58b9-4304-87d1-4c7065af2393
# â•Ÿâ”€f772cb78-6939-4919-b4e0-7749029e9932
# â• â•52f940e7-ba00-4bcb-8f6c-7643ce9b4646
# â• â•0f4b36fc-79bb-46e7-89b2-6dcd6a5c58be
# â• â•fd43367b-a33c-4c09-99e1-ecb7a8f1b293
# â• â•308441b8-b485-433f-b233-91de7da3b7e7
# â•Ÿâ”€d20eba0f-6720-49ec-8f8f-e3f37c555f79
# â• â•06241562-1ab8-4a6f-92ce-722a607a6d6d
# â• â•d20b6d40-0d60-4208-9aa1-76a7493418c5
# â• â•88a3ef1f-a1fb-4965-9fc9-0656573897f1
# â• â•296c3373-2cab-49d6-a173-94e4fec021a8
# â• â•3ff338ac-cabb-4cc1-a326-282562901318
# â• â•11d024b2-2a12-4ebe-9c79-96e19028cdf1
# â• â•c18d944a-56b1-45bd-b974-ad08ad44df9c
# â• â•98aebd62-ba14-481b-9c28-97b9abf4bc4f
# â• â•d56c10a8-4feb-4621-a005-0206cfe0f14a
# â• â•2b6a3941-5d03-4b3a-a8e8-1b3ebbc62f20
# â• â•e5318d56-6289-4451-ac03-f5119223990a
# â• â•4942de35-2a00-4419-b959-b2caa90b361b
# â• â•2051d694-9864-4341-ace4-5ad0184c441a
# â• â•1ca3e292-ce0a-4cef-a08b-6d6628bad2c3
# â• â•ce934e7b-b915-4730-92e4-24f31bd1a745
# â• â•a95f7059-f862-4284-bfb2-17a753136e8b
# â• â•02a4fa64-c9c1-4b1d-9ce3-0128317b360a
# â• â•32fefe1e-7ec0-4756-afa8-70f2d0ae9b25
# â• â•6903aa6c-26ca-4c6f-8dcd-45e9bb7a97b8
# â•Ÿâ”€8009fcbf-892f-445b-a12d-3d2470c71a68
# â• â•b7c80bcd-664a-49e5-a9f4-c84164a213be
# â• â•67188719-ebb0-4dc1-8bcd-36ab2f6f1870
# â•Ÿâ”€312fec2e-94c3-4beb-8e29-9a6a417541c3
# â• â•a6bc299e-deb9-4bb7-8661-52657377988a
# â• â•0fa42aa3-88b8-4786-a619-8bea71f6be8d
# â• â•632a6f89-1528-4142-820e-adeb9d41e48e
# â• â•60d7695c-63cc-4757-807a-a1e294a86360
# â• â•601932f0-a0b1-4fdc-801f-e01b08087e43
# â• â•09272854-da18-417c-ab4c-a3db469f4c92
# â• â•c3be7332-7451-4fba-a459-835b36f420f6
# â• â•dc8afbce-17dd-470b-8ab3-5023605537d6
# â• â•721a32bd-1a1a-4f1f-bf1f-ffc09fcde6e2
# â• â•ce68a804-8cd1-4932-8b76-67b5b863ef75
# â• â•6dc2d711-4286-429f-97df-c360d7846e2b
# â• â•1a936087-22f7-427a-829b-8f328e71046d
# â• â•ce186b02-7e6a-4f46-8d5b-548a3689eebe
# â• â•acbfdf1f-dcd8-4d2c-bcfc-ad221a808134
# â• â•cd71d1f6-059a-4b65-ba40-57c02386998b
# â•Ÿâ”€046e4943-61c2-4258-b876-8b898819cd5b
